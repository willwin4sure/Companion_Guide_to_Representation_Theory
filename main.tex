\documentclass[oneside]{scrbook}
\usepackage[sexy]{evan}
\usepackage[utf8]{inputenc}
\usepackage[all,cmtip]{xy}
\usepackage{todonotes}

\newcommand{\Id}{\text{Id}}

\title{Companion Guide to Rep Theory}
\author{William Yue and Nathan Xiong}
\date{Spring 2022}

\begin{document}

\maketitle

\chapter*{Introduction}
\section{What is this?}
Welcome to our companion guide! This text should serve as a complementary resource for anyone reading Pavel Etingof's classical textbook \textit{Introduction to representation theory}. Throughout this document, you can find solutions to selected problems from the book, as well as some further exposition, explanation, and intuition behind many of the theorems and proofs presented.

Unlike a typical set of ``notes,'' we try to avoid simply ``copying from the textbook.'' Instead, we seek to fill particular holes in the textbook's explanations and proofs to help readers fully understand the material presented.

\section{Why did you make this?}
We are actually currently reading through Etingof's \textit{Introduction to representation theory}, and we often find particular sections or proofs to be confusing or overly terse. If you don't believe us, try reading it yourself. We decided to write this companion guide to help anyone like us who may have gotten stuck while reading through this textbook.

\section{How do I use this?}
The material in this companion guide follows the same order in the table of contents as Etingof's textbook. And typically within each section, the notes obey the order of the textbook's material in that section. Here are some pointers on how this companion guide is formatted.
\begin{itemize}
    \item The comments and explanations of particular theorems, proofs, propositions, etc. are marked and delimited by a bolded subtitle. For example,
    
    \begin{quote}
        \textbf{\sffamily On Theorem 0.3.1.} \quad Part (ii) of Maschke's \dots
    \end{quote}
    \item Intuition, motivation, and other additional exposition are enclosed in a ``remark'' box environment. For example,
    \\
    \begin{quote}
        \begin{remark}
        For irreducible representations $V,W$, \dots
        \end{remark}
    \end{quote}
    \item Solutions to problems are marked and delimited by a bolded subtitle. For example,
    
    \begin{quote}
        \textbf{Problem 0.3.1.} \quad Let $G$ be a finite group \dots
    \end{quote}
    \item Any notes that do not refer to a specific theorem, proof, etc. and don't provide intuition, motivation, etc. but instead simply give additional explanation to plaintext comments in the book are not marked out in the notes. It will be clear from context what it refers to.
\end{itemize}
This guide will (probably) not be a magical cure that makes every line in the textbook clear. You will still need to struggle with the concepts in your mind for them to become clear, but hopefully it is a useful aid. 

\newpage

\setcounter{chapter}{3}

\chapter{Representation of Finite Groups: Basic Results}

\section{Maschke's Theorem}
\paragraph{On Theorem 4.1.1.} Part (ii) of Maschke's theorem specifies an isomorphism of representations $\psi\colon k[G]\to \bigoplus_i \End(V_i)$ defined by $g\to \bigoplus_i g\vert_{V_i}$. That is, we send $g$ to the direct sum of the actions of $g$ on each irrep $V_i$. 

\paragraph{On Proposition 4.1.2.} The proof given by Etingof is quite confusing. 
%First, we know that we can decompose 
%\[k[G]=k\oplus\bigoplus_{i=2}^{r}d_iV_i\]
%where $d_i=\dim V_i$. 
%Now, we consider the space $\Hom_{k[G]}(k,k[G])$ of homomorphisms between the representations $k$ and $k[G]$. By Schur's lemma, it must just be an inclusion of $k$ into $k[G]$ per the decomposition above. If we denote the inclusion map by $\Lambda$, the space of homomorphisms is then just the multiples of $\Lambda$, i.e.,
%\[\Hom_{k[G]}(k,k[G])=k\Lambda.\]
%Let's define $\Lambda$ to be the map defined by $\Lambda(1)=\sum_{g\in G}g$. We claim that this is indeed a homomorphism. It suffices to check that $\Lambda(1)\Lambda(1)=\Lambda(1)$. Indeed by the ``shifting'' trick, we have
%\[\Lambda(1)\Lambda(1)=\left(\sum_{g\in G}g\right)\left(\sum_{h\in G}h\right)\]
We will present a proof, as described in \cite{1133054}, that is easier to understand using the Jacobson radical, which was defined in Chapter 3. Assume FSOC that the characteristic of $k$ divides $|G|$. Consider the element
\[x=\sum_{g\in G}g\in k[G].\]
It satisfies several important properties. First, it is \emph{central}, i.e., for all $g\in G$ we have $gx=x=xg$ by the ``shifting'' trick. Next, it is \emph{nilpotent} because
\[x^2=\left(\sum_{g\in G}g\right)^2=|G|\sum_{g\in G}g=|G|x=0.\]
The ideal generated by $x$ consists just of scalar multiples of $x$ (by centrality), and thus this ideal is nilpotent. It follows that the Jacobson radical $\text{rad}(k[G])$ is nonzero, contradicting the fact that $k[G]$ is semisimple.

\paragraph{Problem 4.1.4.} Let $G$ be a group of order $p^n$. Show that every
irreducible representation of $G$ over a field $k$ of characteristic $p$ is
trivial.
\begin{proof}
We induct on $n$, with the base case proven in Example 4.1.3. Let $Z(G)$ be the center of $G$. By taking mod $p$ on both sides of the class equation, we get that $Z(G)$ is nontrivial. For any irreducible representation $V$, we know that any $g\in Z(G)$ acts via a scalar. By Lagrange's theorem, this scalar is a $p$th root of unity. But by the Frobenius endomorphism, the only $p$th roots of unity over fields of characteristic $p$ are trivial. Hence, any $g\in Z(G)$ just acts via identity.

Now, consider the group $G'=G/Z(G)$ (we can do this since $Z(G)$ is normal). Since $Z(G)$ is nontrivial, the order of $G'$ is $p^m$ for some $m<n$. Let $(V,\rho)$ be an irreducible representation of $G$. Then, let $\rho'$ be the action on $G'$ (which is just $\rho$ after projecting onto $G'$). Since $\img \rho=\img \rho'$, we know that $(V,\rho')$ is an irreducible representation of $G'$. The inductive hypothesis implies $\rho'$ is trivial. However, since we also know that $Z(G)$ must act by identity, we then get that $\rho$ is also trivial, completing the induction.
\end{proof}

\section{Characters}

\paragraph{Example 4.2.3.} Show that if $|G|=0$ in $k$, then the number of isomorphism classes of irreducible representations of $G$ over $k$ is strictly less than the number of conjugacy classes in $G$.

Hint: Let $P=\sum_{g\in G}g\in k[G]$. Then $P^2=0$. So $P$ has zero trace in every finite dimensional representation of $G$ over $k$.

\begin{proof}
As suggested by the hint, let $P=\sum_{g\in G}g$. Then, note that for any $h\in G$, we have
\[hP=\sum_{g\in G}hg=\sum_{\ell \in G}\ell=P.\]
Hence, we have
\[P^2=\sum_{h\in G}hP=|G|P=0.\]
Since $P^2=0$, every eigenvalue of $\rho_V(P)$ is $0$ and $P$ must have zero trace in every finite dimensional representation $V$ of $G$ over $k$. By Theorem 3.6.2., the characters for the irreducible representations are linearly independent. However, we claim that they do not span $(A/[A,A])^*$. This will then prove that the number of isomorphism classes of irreducible representations is less than the number of conjugacy classes. 

Consider the class function $f\in F_c(G,k)$ that sends the identity to $1$ and every other conjugacy class to $0$. Of course, we can linearly extend $f$ to be a function in $(A/[A,A])^*$. Assume FSOC that the characters for the irreducible representations are spanning. Then, there exist scalars $a_1,\ldots,a_n\in k$ such that
\[a_1\chi_{V_1}+a_2\chi_{V_2}+\cdots+a_n\chi_{V_n}=f.\]
Now, if we plug in $P$, we know that $\chi_{V_i}(P)=0$ for $i=1,\ldots,n$. However, by definition we have $f(P)=1$. Contradiction.
\end{proof}

\section{Examples}
\paragraph{Exercise 4.3.1.} Show that the 2-dimensional irreducible representation of $Q_8$ can be realized in the space of functions $f\colon Q_8\to \CC$ such that $f(ig) = \sqrt{-1}f(g)$ (the action of $G$ is by right multiplication, $g\circ f(x) = f(xg)$).

\begin{proof}
The function $f$ is completely determined by $f(1)$ and $f(j)$. Indeed, we then have
\begin{align*}
    f(i) &= \sqrt{-1}f(1)\\
    f(-1) &= -f(1)\\
    f(-i) &= -\sqrt{-1}f(1)\\
    f(k) &= \sqrt{-1}f(j)\\
    f(-j) &= -f(j)\\
    f(-k) &= -\sqrt{-1}f(j)
\end{align*}
First, note that for any $g\in Q_8$, 
\[\rho(g)f(1)=f(g).\]
And, we have
\begin{align*}
    \rho(1) f(j) &= f(j)\\
    \rho(-1) f(j) &= f(-j) = -f(j)\\
    \rho(i) f(j) &= f(-k)=-\sqrt{-1}f(j)\\
    \rho(j) f(j) &= f(-1) = -f(1)\\
    \rho(k) f(j) &= f(i) =\sqrt{-1}f(1)
\end{align*}
Hence, in the vector space $\CC^2$ spanned by the values of $f(1)$ and $f(j)$, we have $\rho(-1)=-\text{Id}$ and 
\begin{align*}
    \rho(i) &= \begin{pmatrix}
    \sqrt{-1} & 0\\
    0 & -\sqrt{-1}\end{pmatrix}\\
    \rho(j) &= \begin{pmatrix}
    0 & -1\\
    1 & 0\end{pmatrix}\\
    \rho(k) &= \begin{pmatrix}
    0 & \sqrt{-1}\\
    \sqrt{-1} & 0\end{pmatrix}
\end{align*}
This representation is irreducible (there is no common eigenvalue between these three matrices), so this is isomorphic to the previously described two-dimensional irrep of $Q_8$. 
\end{proof}

\section{Duals and Tensors of Representations}
If $V$ is a representation of $A$ then $V^*$ is a representation of the opposite algebra $A^{\text{op}}$ (see Definition 3.3.2). Then, we can define a representation $V^*$ on $A$ if there is some antiautomorphism on $A$ (that is, an isomorphism $A\to A^{\text{op}}$). Indeed, for group algebras we have $k[G]\cong k[G]^{\text{op}}$ by the isomorphism $g\mapsto g^{-1}$.

Then, we define $(\rho_{V^*}(g)\lambda)v=\lambda(\rho_V(g^{-1})v)$.

\begin{remark}
If we have representations $V,W$ of $A$, when can we define representations $V^*$ or $V\otimes W$ of $A$?
\begin{enumerate}[(1)]
    \item To define a representation $V^*$ we need a homomorphism of algebras $s:A\to A^{\text{op}}$. Then we can define $(a\lambda)(v)=\lambda(s(a)v)$
    \item To define a representation $V\otimes W$ we need a homomorphism of algebras $\Delta:A\to A\otimes A$. Then we can define
    \[a(v\otimes w)=\Delta(a)(v\otimes w)\]
\end{enumerate}
If $A$ has both such functions (with some other minor conditions) then we call it an \vocab{Hopf algebra}.

\begin{example}
$k[G]$ is a Hopf algebra by choosing $s:g\mapsto g^{-1}$ and $\Delta:g\mapsto g\otimes g$.
\end{example}

\begin{example}
For a Lie algebra $\mathfrak{g}$, the universal enveloping algebra $\mathcal{U}(\mathfrak{g})$ is a Hopf algebra by choosing $s:x\mapsto -x$ and $\Delta:x\mapsto x\otimes 1+1\otimes x$ (Leibniz rule).
\end{example}
Most algebras are not Hopf algebras.
\end{remark}

Why do we need the inverse when we define the dual representation? It's to flip us to the opposite algebra (similarly to why we introduce a negative sign when defining the dual of a Lie algebra representation); indeed, we can check that we get an issue if we omitted it. If we set
\[g\lambda(v)=\lambda(gv),\]
then
\[(hg)\lambda(v)=\lambda((hg)v).\]
Yet
\[h(g\lambda(v))=g\lambda(hv)=\lambda((gh)v).\]

\section{Orthogonality of Characters}

A \vocab{Hermitian} inner product is a sesquilinear form (linear in first argument and antilinear in second argument) such that $\langle u,v\rangle=\ol{\langle v,u\rangle}$, where the bar represents conjugation.

\paragraph{On Theorem 4.5.1.} We've shown that the characters $\chi_V$ over irreducible $V$ form a basis of the space $F_c(G,\CC)$ of class functions; now after defining the inner product we're interested in proving that they form an \textit{orthonormal} basis under this inner product.

The first key observations in the proof are that $P$ is idempotent and that it is central. Since it is central, it acts by a scalar on every irrep $X$. However, since $P^2=P$, it either acts by $\Id$ or $0$. The classification, as written in the book, is
\[P|_X=\begin{cases}
\Id & \text{if } X=\CC,\\
0 & \text{if } X\neq \CC.
\end{cases}\]
where $\CC$ is the trivial representation of $G$, which sends every $g\in G$ to the identity map on $\CC$. Now, the cool property of $P$ is that $gP=P$ for all $g\in G$ (this is a ``shifting'' trick). So, if $P|_X=\Id$, then $v=Pv=gPv=gv$, so every element $g$ stabilizes every vector $v$. Hence $X$ is then the trivial representation $\CC$ (it must be one-dimensional so it's $\CC$).

In other words, if $X\neq \CC$, then pick some $g$ such that $\rho(g)\neq \Id$. Then
\[\rho(P)=\rho(gP)=\rho(g)\rho(P).\]
But since $\rho(g)\neq \Id$, we need $\rho(P)=0$.

By Maschke's theorem, every representation $V$ can be decomposed into a direct sum of irreducibles $\bigoplus_i V_i^{n_i}$, then note that $P$ acts by zero on anything not in $\mathbb{C}^{n_1}$ (where $V_1=\CC$), and by identity on $\CC^{n_1}$. Now $P$ is a $G$-invariant projector from $V$ to the subspace $V^G$ of $G$-invariants in $V$. That is, the image of $P$ in $V$ is $V^G:=\{v:gv=v\, \forall g\}\cong \CC^{n_1}$, which is easy to check. 

Therefore, $\Tr|_{V\otimes W^*}(P)$ is equal to the value of $n_1$ (i.e. the number of trivial irreps $\CC$) inside $V\otimes W^*$. From a more abstract view, it is equal to the dimension of $V^G$. Then,
\[(\chi_V,\chi_W)=\dim(V\otimes W^*)^G=\dim \Hom_\CC(W,V)^G=\dim \Hom_G(W,V).\]
To get the second equality, we want to prove that $V\otimes W^{*}\cong \Hom_{\CC}(W,V)$ as representations (we know that there is a canonical isomorphism of vector spaces, but we need to know that $G$ acts the same for the subspace of $G$-invariants to be isomorphic). Note that here, $G$ acts on $\Hom_{\CC}(W,V)$ by conjugation, i.e., $A\mapsto gAg^{-1}$. This is not too hard to understand though. By the definition of tensor and dual representations, an element $g$ acts via $v\otimes \lambda \to gv\otimes g\lambda=gv\otimes \lambda g^{-1}$. This essentially corresponds to conjugation on the linear map corresponding to $v\otimes \lambda$, which is how $G$ acts on $\Hom_{\CC}(W,V)$. 

The last equality becomes obvious once we understand that $G$ acts on $\Hom_{\CC}(W,V)$ via conjugation. Indeed, for a linear map $A$ to be $G$-invariant, it means that $A=gAg^{-1}\iff gA=Ag$ for all $g\in G$. This means that $A$ is a homomorphism (it commutes with the action of $G$). Equivalently we are saying that $A$ respects the commutative diagram
\[\xymatrix{
W \ar[r]^A &V\ar[d]^g\\
W \ar[u]^{g^{-1}} \ar[r]^A &V}
\]
Hence $\Hom_\CC(W,V)^G\cong \Hom_G(W,V)$, so the final equality is valid.

\paragraph{Problem 4.5.2.} Let $G$ be a finite group. Let $V_i$ be the irreducible complex representations of $G$. For every $i$, let
\[\psi_i = \frac{\dim{V_i}}{|G|} \sum_{g \in G} \chi_{V_i} (g) \cdot g^{-1} \in \mathbb{C}[G].\]
\begin{enumerate}[(i)]
    \item Prove that $\psi_i$ acts on $V_j$ as the identity if $j=i$, and as the null map if $j\neq i$.
    \item Prove that $\psi_i$ are \textbf{idempotents}; i.e., $\psi_i^2=\psi_i$ for any $i$, and $\psi_i\psi_j=0$ for any $i\neq j$.
\end{enumerate}
Hint: In (i), notice that $\psi_i$ commutes with any element of $k[G]$ and thus acts on $V_j$ as an intertwining operator. Corollary 2.3.10 thus
yields that $\psi_i$ acts on $V_j$ as a scalar. Compute this scalar by taking
its trace in $V_j$.

\begin{proof}
\begin{enumerate}[(i)]
    \item Note first that $\psi_i$ commutes with any element of $k[G]$ i.e. $\psi_i(av) = a\psi_i(v)$ for any $a \in k[G]$, so $\psi_i$ is an intertwining operator. Then by Schur's lemma it acts by a scalar on any irrep. Now notice that 
    \begin{align*}
        \Tr\rvert_{V_j}(\psi_i) &= \frac{\dim{V_i}}{|G|} \sum_{g \in G} \chi_{V_i} (g) \cdot \Tr\rvert_{V_j}(\phi(g^{-1})) \\
        &= \frac{\dim{V_i}}{|G|} \sum_{g \in G} \chi_{V_i} (g) \cdot \chi_{V_j}(g^{-1}) \\
        &= \frac{\dim{V_i}}{|G|} \sum_{g \in G} \chi_{V_i} (g) \cdot \chi_{V_j^{*}}(g) \\
        &= \frac{\dim{V_i}}{|G|} \sum_{g \in G} \chi_{V_i \otimes V_j^{*}} (g) \\
        &= \frac{\dim{V_i}}{|G|} \sum_{g \in G} \dim\Hom_G(V_j, V_i) \\
        &= \frac{\dim{V_i}}{|G|} \sum_{g \in G} (\chi_{V_j}, \chi_{V_i}).
    \end{align*}

Since $(\chi_{V_j}, \chi_{V_i}) = 1$ for $i=j$ and $0$ otherwise, the above expression is equivalent to $\dim V_i$ for $i=j$ and $0$ otherwise. Then $\psi_i$ operates by the scalar $\frac{\Tr\rvert_{V_j}(\psi_i)}{\dim V_i}$, which equals $\frac{\dim V_i}{\dim V_i} = 1$ when $i=j$ and $0$ when $i \neq j$, as desired.
    \item By Maschke's theorem, we have $k[G]\cong\bigoplus_{i}\End V_i$ with isomorphism given by $g\to \bigoplus_i g\vert_{V_i}$. From part (i), we know that $\psi_i$ acts by identity on $V_i$ and $0$ on everything else. So $\psi_i$ is just sent to the map which is identity on $V_i$ and null everywhere else. If we want to think about this as a matrix, there is an identity block for $V_i$ and everything else is $0$. Then, it becomes obvious that in $k[G]$, we have $\psi_i^2=\psi_i$ and $\psi_i\psi_j=0$ for $i\neq j$. In this way, note that the $\psi_i$ almost act as ``indicators'' for the irreps of $k[G]$. 
\end{enumerate}
\end{proof}

\paragraph{On Problem 4.5.2.} This problem is generalizing the idea of $P$ and $\CC$ in the proof of Theorem 4.5.1. to a projector $\psi_i$ from $V$ to any irrep $V_i$. We are showing that it acts by identity on $V_i$ and zero on any other irrep.

\paragraph{On Remark 4.5.3.} It is not too difficult to check that central functions $f\colon G\to \CC$ (functions on the conjugacy classes of $G$) correspond one-to-one with central elements
\[\pi=\sum_{g\in G}f(g)g^{-1}\]
in $\CC[G]$. What do idempotent central elements $\pi$ correspond to? Well, if $\pi^2=\pi$, then
\[\pi^2=\sum_{g,h\in G}f(g)f(h)g^{-1}h^{-1}=\sum_{z\in G}\left(\sum_{hg=z}f(h)f(g)g^{-1}h^{-1}\right)=\sum_{z\in G}(f*f)(z)z^{-1}=\pi.\]
Since we have two basis decompositions in $\CC[G]$ (the $g\in G$ are the basis elements of $\CC[G]$), the coefficients match, implying $f*f=f$.

Therefore, the idempotent central elements $\psi_i$ of the group algebra $\CC[G]$ in Problem 4.5.2. correspond to the central functions $\tilde{\chi_i}$ which are solutions to $f*f=f$. They are given by
\[\sum_{g\in G}\tilde{\chi_i}(g)g^{-1}=\frac{\dim V_i}{|G|}\cdot\sum_{g\in G}\chi_i(g)g^{-1}.\]
Now note that
\[\tilde{\chi_i}(g)=\frac{\dim V_i}{|G|}\cdot \chi_i(g).\]
In particular,
$\tilde{\chi_i}(1)=\frac{(\dim V_i)^2}{|G|}$, so
\[\chi_i(g)=\sqrt{\frac{|G|}{\tilde{\chi_i}(1)}} \tilde{\chi_i}(g),\]
as written in the text.

\paragraph{On Theorem 4.5.4.} Here, instead of summing over group elements, we sum over irreps. Then, note that for any irrep $V$,
\begin{align*}
    \chi_V(g)\chi_{V^*}(h)&=\Tr(\rho_V(g))\Tr((\rho_V(h)^*)^{-1})\\
    &= \Tr\vert_{V\otimes V^*}(\rho_V(g)\otimes (\rho_V(h)^*)^{-1})
\end{align*}
where we are using the property of matrices/linear maps that $\Tr(S\otimes T)=\Tr(S)\Tr(T)$. Then, by summing over all irreps (the characters are additive), we have
\[\sum_V \chi_V(g)\chi_{V^*}(h)=\Tr\vert_{\bigoplus_V V\otimes V^{*}}(\rho_V(g)\otimes (\rho_{V}(h)^*)^{-1}).\]
Then, by identifying $V\otimes V^*$ with $\End V$, the element $\rho_V(g)\otimes (\rho_V(h)^*)^{-1}$ corresponds to the map $x\to gxh^{-1}$ (the use of the dual in $(\rho_V(h)^*)^{-1}$ is essentially telling $\rho_V(h)^{-1}$ to act on the right). Hence, we have
\[\sum_V\chi_V(g)\chi_{V^*}(h)=\Tr\vert_{\bigoplus_V \End V}(x\to gxh^{-1}).\]
And by Maschke's theorem $\bigoplus_V \End V=\CC[G]$, so finally
\[\sum_V\chi_V(g)\chi_{V^*}(h)=\Tr\vert_{\CC[G]}(x\to gxh^{-1}).\]
Clearly, if $g$ and $h$ are not conjugate, there is no $x\in G$ such that $x=gxh^{-1}$. Hence, every diagonal entry is in the matrix is zero (basically, the value of $gxh^{-1}$ cannot be $x$ for each $x$; it's some other element in the group). So, the trace must be zero. If $g$ and $h$ are conjugate, i.e., there is $y\in G$ where $h=y^{-1}gy$, then for each $x\in Z_g$, note that $z=xy$ satisfies $z=gzh^{-1}$. It's easy to check that all $z$ for which $z=gzh^{-1}$ are of this form, so it follows that the trace is equal to $|Z_g|$ when $g$ and $h$ are conjugate.

\paragraph{On Remark 4.5.5.} We're taking the dot product of any two rows, using the Hermitian form $(v,w)=\sum_i v_i\ol{w_i}$:
\[\sum_{g\in \mathcal{C}(G)}\frac{\chi_V(g)}{\sqrt{|Z_g|}}\cdot\ol{\frac{\chi_W(g)}{\sqrt{|Z_g|}}}=\sum_{g\in\mathcal{C}(G)}\frac{\chi_V(g)\ol{\chi_W(g)}}{|Z_g|}=\frac{1}{|G|}\sum_{g\in G}\chi_V(g)\ol{\chi_W(g)}\]
where $\mathcal{C}(G)$ is the set of conjugacy classes of $G$, noting that the size of the conjugacy class of $g$ is $|G|/|Z_g|$ and that characters are central functions. Therefore, the rows of the matrix are orthornomal by Theorem 4.5.1; so the columns are too (a property of unitary matrices), giving Theorem 4.5.4 immediately.

This matrix captures these two ``dual'' theorems perfectly: they both simply state that this matrix is unitary.

\section{Unitary representations. Another proof of
Maschke’s theorem for complex
representations}

\paragraph{On Definition 4.6.1.} Essentially, a \vocab{unitary} finite dimensional representation is one such that any ``button press" of $G$ acting on $V$ transforms $V$ in a way that doesn't disrupt the inner product/definition of distances. For example, we could have $G=S_3$ and $V=\mathbb{C}^2$ where group elements act by symmetries of an equilateral triangle; since all elements of $G$ here act by just rotation/reflection, the inner product isn't disturbed.

\paragraph{On Theorem 4.6.2.} First, there always exists some positive definite form $B$ by choosing a basis $v_1,\ldots,v_n$ of $V$ and setting $B(v,w)=\sum_i a_i\ol{b_i}$, where $v=\sum_i a_iv_i$ and $w=\sum_i b_iv_i$. Then, the fact that $\ol{B}$ is $G$-invariant follows from the ``shifting'' trick. 

Now, we prove that if $B_1$ and $B_2$ are $G$-invariant, positive definite Hermitian forms, then there exists a homomorphism $A\colon V\to V$ such that $B_1(v,w)=B_2(Av,w)$. First, since $B_1$ and $B_2$ are positive definite, they are also nondegenerate. A nondegenerate form is one such that if $B(v,w)=0$ for every $w\neq 0$, then $v=0$. This is obvious by the definition of a positive definite form (if there were a bad $v\neq 0$, then take $w=v$).

For any $v$, there exists a $v'$ such that $B_1(v,w)=B_2(v',w)$. For an explicit construction, we have $v'=(B_1B_2^{-1})^Tv$, where $B_1$ and $B_2$ are the associated matrices (the inverse exists because the matrix is nondegenerate). Furthermore, the $v'$ is unique. Indeed, if we had $B_2(v_1',w)=B_2(v_2',w)$, then $B_2(v_1'-v_2',w)=0$ for every $w\neq 0$, implying $v_1'=v_2'$ by nondegeneracy of $B_2$. So, define the map $A\colon V\to V$ by $Av=v'$. Since $B_1$ and $B_2$ are linear in the first argument, $A$ is also linear. 

Finally, using the $G$-invariance of $B_1$ and $B_2$, we have
\[B_1(gv,w)=B_2(Agv,w).\]
On the other hand,
\[B_1(gv,w)=B_1(v,g^{-1}w)=B_2(Av,g^{-1}w)=B_2(gAv,w).\]
So, we have $B_2(Agv,w)=B_2(gAv,w)$ for every $w$. By the nondegeneracy of $B_2$, we get $Agv=gAv$ for all $g,v$ which implies $A$ is a homomorphism. 

\begin{remark}
There is a typo in the book's proof to Theorem 4.6.2. The representations $B_1,B_2$ are assumed to be $G$-invariant.
\end{remark}

After the proof, the author remarks that the dual representation $V^*$ is then naturally isomorphic to the \textbf{complex conjugate representation} $\ol{V}$. Note that $\ol{V}$ is essentially just $V$ (same addition and action by $G$) but scalar multiplication multiplies by the conjugate instead, e.g., $z\cdot v=\ol{z}v$. 

Given a \textit{bilinear} form, we have a natural map from $V$ to $V^*$ by $v\mapsto \lambda_v(\bullet)=(\bullet, v)$. However, given a \textit{sesquilinear (Hermitian)} form, we have that $(\bullet, av)=\ol{a}(\bullet, v)$, so we need to use the complex conjugate representation $\ol{V}$ to construct an isomorphism of vector spaces from $\ol{V}$ to $V^*$. We can further check that this is an isomorphism of representations due to the $G$-invariance of the Hermitian form:
\[gv\mapsto (\bullet, gv)=g\lambda_v\]
since $(u,gv)=(g^{-1}u,v)$ and $g\lambda(u)=\lambda(g^{-1}u)$ in the definition of the dual representation.

\paragraph{On Theorem 4.6.3.} The orthogonal complement $W^\perp$ is the set of vectors $u$ in $V$ such that the inner product of $u$ with any $w\in W$ is 0 (i.e. $u$ is orthogonal to all $w$). $W^\perp$ is a subrepresentation because for any $u\in W^\perp$, we can show that $gu\in W^{\perp}$ since
\[(gu,w)=(u,g^{-1}w)=0,\]
where the first equality is due to the $G$-invariance of the form, and the second equality holds since $g^{-1}w\in W$ as $W$ is a subrepresentation.

\begin{remark}
The book has a typo in the proof to Theorem 4.6.3. It should say that $W^\perp$ is a subrepresentation of $V$, not $W$.
\end{remark}

% unitary like each button press $g$ being rotations of $V$

\section{Orthogonality of matrix elements}
\paragraph{On Proposition 4.7.1.} Essentially, this proposition claims that if we take all the matrix elements (which are thought of as functions $G\to \CC$) over all irreps of $G$, then this collection of functions form an orthogonal basis for $F(G,\CC)$. 

In the proof, Etingof implicitly defines a Hermitian product on the dual space $W^*$. This is done by the following definition $(w_i^*,w_j^*)=\ol{(w_i,w_j)}$. Note that we need the conjugate to ensure linearity in the first argument: recall from the previous section that scalars act by conjugate multiplication in the dual space since $V^*\cong \ol{V}$. Furthermore, due to this isomorphism we have $xw_i^*=(xw_i)^*$ which implies the first equality $(xw_i^*,w_j^*)=\ol{(xw_i,w_j)}$. 

Etingof also implicitly defines a Hermitian product on the tensor product $V\otimes W^*$. The definition here is the natural one \[(v_i\otimes w_j^*,v_{i'}\otimes w_{j'}^*)=(v_i,v_{i'})(w_j^*,w_{j'}^*)\]
extended linearly. 

Then, recall from the proof of Theorem 4.5.1. that the image of the operator $P$ in any representation is just the trivial representations $\CC$ ``contained'' in the representation. However, when $V\neq W$, we found out that this space in $V\otimes W^*$ is empty by Schur's lemma. Thus, $P$ must then map $v_i\otimes w_j^*$ to $0$, so the inner product is just $0$ for $V\neq W$, proving part (i).  

When $V=W$, recall from the proof of Theorem 4.5.1. that $V\otimes V^*$ contains exactly one copy of $\CC$ (since $V$ is irreducible). So, we have a decomposition of representations $V\otimes V^*=\CC\oplus L$ (the decomposition into subrepresentations is valid since $V\otimes V^*$ is completely reducible). The trivial representation $\CC$ consists of
\[\CC=\text{span}\left\{\sum_{k}v_k\otimes v_k^*\right\}\]
because these are the $G$-invariant terms in $V\otimes V^*$. Indeed, if we think about elements of $V\otimes V^*$ as linear maps from $V$ to itself, then $\sum_k v_k\otimes v_k^*$ is the identity map (think about the matrix), so any $g$ will leave it unchanged (recall that $g$ acts on matrices via conjugation). 

Then, it turns out that $L$ is the subrepresentation of traceless matrices (this is a subrep because conjugation preserves trace). Thus, because $P$ projects onto $\CC$, it basically takes a matrix $M$ to the matrix $\Tr(M)\cdot\Id$. This implies the first result, i.e., the projection of $v_i\otimes v_{i'}^*$ is
\[\frac{\delta_{ii'}}{\dim V}\sum_k v_k\otimes v_k^*.\]
Then, the inner product on $V\otimes V^*$ (thought of as matrices) is what we would expect: multiply corresponding entries in the matrix and sum over all of them. This is also known as the Frobenius inner product of matrices. With this in mind, we get the final equality
\[(P(v_i\otimes v_{i'}^*), v_j\otimes v_{j'}^*)=\frac{\delta_{ii'}\delta_{jj'}}{\dim V}.\]
Finally, note that these matrix elements are all linearly independent. They form a basis by the ``sum of squares'' formula. 

\begin{remark}
The motivation for studying the space $F(G,\CC)$ and defining this orthogonal basis is that, as mentioned in Remark 4.5.3., the space $F(G,\CC)$ equipped with the convolution product
\[(f*g)(z)=\sum_{x,y\in G:xy=z}f(x)g(y)\]
is actually just the group algebra $\CC[G]$ if we write each element of $\CC[G]$ as $\sum_{g\in G}f(g)g$ for some $f\in F(G,\CC)$. 

Furthermore, recall that by Maschke's theorem we have the matrix sum decomposition
\[\CC[G]=\bigoplus_{V}\End(V)\]
where we sum over all irreps. We would like to have some sort of basis to this space which just consists of a set of matrices for each irrep. The matrix elements give us exactly this, and this basis is also orthogonal with respect to the inner product.
\end{remark}

\section{Character tables, examples}
The textbook gives the following example as the character table for the symmetric group $S_3$. 
\[\begin{tabular}{|c|c|c|c|}
\hline
    $S_3$ & $\Id$ & $(12)$ & $(123)$ \\
    \hline
    \# & 1 & 3 & 2\\
    \hline
    $\CC_+$ & 1 & 1 & 1\\
    \hline
    $\CC_-$ & 1 & $-1$ & 1\\
    \hline
    $\CC^2$ & 2 & 0 & $-1$\\
    \hline
\end{tabular}\]
Let's dissect all the information in this character table.
\begin{itemize}
    \item The first row shows the group $S_3$, as well as the three conjugacy classes of $S_3$. The first conjugacy class is $\Id$ and just includes the identity permutation. The second conjugacy is $(12)$ and contains the permutations $(12),(23),(31)$, i.e., these all just swap two elements. The final conjugacy class is $(123)$ and contains the permutations $(123)$ and $(132)$. 
    \item The second row shows the number of elements in each conjugacy class. As we computed above, there is $1$ element in $\Id$, $3$ elements in $(12)$, and $2$ elements in $(123)$. Recall that the character is a class function, so it will take on equal values for all elements in the same conjugacy class.
    \item Starting from the third row, we start to encounter irreps. The first irrep is the trivial representation $\CC_+$. This is the representation on $\CC$ which sends each permutation to identity (multiplication by $1$). So, the trace is just $1$ for each conjugacy class.
    \item The fourth row is the ``sign-representation'' on $\CC$, i.e., it sends each permutation to the sign of the permutation. The identity permutation and $(123)$ conjugacy class are both even permutations so they act by multiplying by $1$. The $(12)$ conjugacy class, on the other hand, acts by multiplying by $-1$. 
    \item The fifth row is the two-dimensional representation (recall that the dimensions of the irreps can be determined from the ``sum of squares'' formula). The $\Id$ permutation acts via identity on $\CC^2$, so its trace is $2$. Then, recall from the second example in Section 4.3. that this representation is generated by
    \[\rho((12))=\begin{pmatrix}
    1 & 0\\
    0 & -1\end{pmatrix}, \qquad \rho((123))=\begin{pmatrix}
    \cos 120^{\circ} & -\sin 120^{\circ}\\
    \sin 120^{\circ} & \cos 120^{\circ}\end{pmatrix}.\]
    So, the character for the $(12)$ conjugacy class is $1-1=0$, and the character for the $(123)$ conjugacy class is $2\cos 120^{\circ}=-1$. This gives us the final row of the character table. 
    \item Furthermore, Theorem 4.5.1. and Theorem 4.5.4. imply particular orthogonality relations. If we look at the \emph{rows}, we apply the usual inner product, while multiplying each term by the number of elements in the conjugacy class. For example,
    \[\langle \CC_+,\CC_-\rangle = \frac{1}{6}(1\cdot 1+3\cdot (-1)+2\cdot 1)=0\]
    and
    \[\langle \CC^2,\CC^2\rangle = \frac{1}{6}(1\cdot 4+3\cdot 0+2\cdot 1)=1.\]
    If we look at the \emph{columns}, we just apply the usual inner product. For example,
    \[\langle \Id, (123)\rangle = 1\cdot 1+1\cdot 1+2\cdot (-1)=0.\]
\end{itemize}

\section{Computing tensor product multiplicities using character tables}
The formula
\[V_i\otimes V_j=\sum N_{ij}^kV_k, \qquad N_{ij}^{k}=(\chi_i\chi_j,\chi_k)\]
where the $V_k$ are irreps, just follows from characters. The character of $V_i\otimes V_j$ is just $\chi_i\chi_j$, and we'd like to express this function in the orthonormal basis of the characters of irreps. This is done using orthogonal projections (taking the inner product with each basis function). 
\[\chi_i\chi_j=\sum (\chi_i\chi_j,\chi_k)\chi_k.\]
Finally, since the character of a decomposition of representations is the sum of the characters, this gives us a decomposition of $V_i\otimes V_j$ into irreps.

\section{Frobenius determinant}
\paragraph{On Lemma 4.10.3.} 
The proof given by Etingof works, however, we don't see why the determinant polynomial $t^n-(-1)^nx_1\cdots x_n$ is ``obviously'' irreducible. We will give another proof, as described in \cite{147085}.

Let $D$ denote the determinant, which is a polynomial in $\{x_{ij}\}$. In particular, note that each $x_{ij}$ has degree one in $D$ (expand by minors). Assume FSOC that $D$ can be decomposed $D=fg$ for nonconstant polynomials $f$ and $g$. Consider the $x_{11}$ variable. It can only appear in one of $f$ and $g$, so assume that it appears with degree one in $f$. Next, consider the variable $x_{1j}$. It must also appear with degree one in $f$, because otherwise we would have a monomial with $x_{11}x_{1j}$ in $D$, which cannot happen (expand by minors). Similarly, all the variables $x_{j1}$ are in $f$. Then if $x_{j1}$ is in $f$, the variables $x_{jk}$ must be in $f$. Indeed, all $x_{ij}$ have degree one in $f$, so $g$ is constant, contradiction!

% We claim this polynomial in $t,x_1,\ldots,x_n$ is irreducible. Suppose otherwise, i.e., $\det X=fg$ for nonconstant polynomials $f$ and $g$. Plug in all $x_i=0$ except for $x_j$. Then, $\det X$ becomes $t^n$, so $f$ and $g$ must be polynomials in $t$. In particular, if there is some monomial in $f$ or $g$ with one of $x_1,\ldots,x_n$, it must contain all of $x_1,\ldots,x_n$. However, this quickly gives a degree contradiction if $f$ and $g$ are both nonconstant in $x_1,\ldots,x_n$. It then follows that one of $f$ and $g$ is constant. 

\paragraph{On Theorem 4.10.2.} We begin by defining the operator-valued polynomial
\[L(\mathbf{x})=\sum_{g\in G}x_g\rho(g).\]
We should think of an operator-valued polynomial just as a matrix which has linear functions of the variables $x_1,\ldots,x_g$ in its entries. For example, see the matrix $X$ above. Then, the equality
\[\sideset{}{_V}\det L(\mathbf{x})=\prod_{i=1}^{r}(\sideset{}{_{V_i}}\det L(\mathbf{x}))^{\dim V_i},\]
follows because 
\[L\vert_V=\bigoplus_{i=1}^r L\vert_{V_i^{\oplus \dim V_i}}\]
which is just Maschke's theorem (the determinant of a direct sum is the product of the determinants). So it leaves to show that each $\sideset{}{_{V_i}}\det L(\mathbf{x})$ is an irreducible polynomial and that none of these polynomials are proportional to one another. 

As the book says, we construct $r$ sets of bases, one for each $V_i$, and we denote this collection by $\{e_{im}\}$. Then, we can consider the matrix units (essentially the elementary matrices) in this basis, and we denote this collection by $\{E_{i,jk}\}$. Since $\CC[G]\cong \bigoplus_{i=1}^{r}\End(V_i)$, these matrix units $\{E_{i,jk}\}$ form a basis for $\CC[G]$ (check that the dimensions match up by the ``sum of squares'' formula!). So, we can rewrite $L$ under a change of basis
\[L(\mathbf{x})\vert_{V_i}=\sum_{j,k}y_{i,jk}E_{i,jk}.\]
Since this is a change of basis, the coefficients change by some invertible linear transformation. By the lemma above, we know that $\sideset{}{_{V_i}}\det L(\mathbf{x})$ is irreducible in the $y_{i,jk}$. It follows that the polynomial must be irreducible in the $x_1,\ldots,x_g$ (if not we could just do a change of basis to get a contradiction). Furthermore, these determinant polynomials are pairwise nonproportional because they depend on different collections $y_{i,jk}$ (again, just do a change of basis to get a contradiction). 

\setcounter{chapter}{5}

\chapter{Quiver Representations}

\setcounter{section}{1}

\section{Indecomposable representations of the quivers $A_1,A_2,A_3$}
The main idea of finding these indecomposable representations is to just keep peeling away specific subspaces (associated to the canonical maps) from our quiver representation until we end up with an indecomposable representation. 

As an example, the quiver
\[\xymatrix{
\overset{1}{\bullet}
\ar[r] & \overset{0}{\bullet}\ar[r]  &
\overset{1}{\bullet} 
}\]
is not indecomposable because we can write
\[\xymatrix{\overset{1}{\bullet}\ar[r] & \overset{0}{\bullet}\ar[r] & \overset{1}{\bullet}}=\xymatrix{\overset{1}{\bullet}\ar[r] & \overset{0}{\bullet}\ar[r] & \overset{0}{\bullet}}\oplus\xymatrix{\overset{0}{\bullet}\ar[r] & \overset{0}{\bullet}\ar[r] & \overset{1}{\bullet}}\]
because all the maps in the original quiver must be zero maps. 

\section{Indecomposable representations of the quiver $D_4$}
The setup for finding indecomposable representations of $D_4$ is just a much longer and more involved equivalent to that for quivers $A_1,A_2,A_3$. After peeling away enough subspaces, we eventually reach a quiver
\[\xymatrix{\underset{V_1}{\bullet}\ar@{^{(}->}[r] & \overset{V}{\bullet} & \ar@{_{(}->}[l] \underset{V_3}{\bullet}\\
& \underset{V_2}{\bullet}\ar@{_{(}->}[u] &
}\]
with the conditions that
\begin{enumerate}[(1)]
    \item $V_1+V_2+V_3=V$
    \item $V_1\cap V_2=0,\quad V_1\cap V_3=0,\quad V_2\cap V_3=0.$
    \item $V_1\subseteq V_2\oplus V_3,\quad  V_2\subseteq V_1\oplus V_3,\quad V_3\subseteq V_1\oplus V_2.$
\end{enumerate}
The final condition may seem confusing at first, but one example for which all three conditions are satisfied is when $V_1,V_2,V_3$ are distinct lines through the origin in $\RR^2$ and $V=\RR^2$. In this case, the intersection of any two vector spaces is zero, and the direct sum of any two vector spaces is just $\RR^2$. 

Etingof first claims that $V_1\oplus V_2=V$. Since $V_1\cap V_2=0$, we have $V_1\oplus V_2=V_1+V_2$, and since $V_3\subseteq V_1\oplus V_2=V_1+ V_2$, we have $V_1+V_2=V_1+V_2+V_3=V$, as desired. The other equalities follow symmetrically, and the dimension relations for $V_1$, $V_2$, $V_3$, and $V$ follow easily. 

Zooming in on the condition $V_3\subseteq V_1\oplus V_2$, we know that we can write every $x\in V_3$ as $x=(x_1,x_2)$ for unique $x_1\in V_1, x_2\in V_2$ (even though we write it as an ordered pair, we should think of $(x_1,x_2)$ as the sum of $x_1+x_2$). Then, there are projection operators
\begin{align*}
    B_1\colon V_3\to V_1,&\quad x=(x_1,x_2)\to x_1,\\
    B_2\colon V_3\to V_2,&\quad x=(x_1,x_2)\to x_2.
\end{align*}
We claim that $B_1$ and $B_2$ are both isomorphisms; it suffices to check that they are both injective. Consider the kernel of $B_1$, which consists of all $x=(0,x_2)=0+x_2=x_2$. However, since $V_2\cap V_3=0$, this kernel is trivial. Similarly, since $V_1\cap V_3=0$, the kernel of $B_2$ is trivial. Hence, both maps are isomorphisms. This allows us to define the composed isomorphism $A\colon V_1\to V_2$ by $A=B_2\circ B_1^{-1}$. In particular, for $x=(x_1,x_2)$, we always have $x_2=Ax_1$, and for any $x_1\in V_1$, the pair $(x_1,Ax_1)$ always belongs to $V_3$. 

So now, consider a basis $e_1,\ldots, e_n$ for $V_1$, i.e.,
\[V_1=\CC e_1\oplus \CC e_2\oplus\cdots\oplus \CC e_n.\]
Since $A\colon V_1\to V_2$ is an isomorphism, it follows that $Ae_1,\ldots,Ae_n$ is a basis for $V_2$, i.e.,
\[V_2=\CC Ae_1\oplus \CC Ae_2\oplus\cdots\oplus \CC Ae_n.\]
Finally, consider the elements $(e_1,Ae_1),\ldots,(e_n,Ae_n)$ in $V_3$. It is easy to check that they must form a basis for $V_3$ (they are clearly spanning), i.e.,
\[V_3=\CC (e_1+Ae_1)\oplus \CC (e_2+Ae_2)\oplus \cdots\oplus \CC (e_n+Ae_n).\]
From this decomposition of $V_1$, $V_2$, and $V_3$, we can just analyze one component, i.e., $\CC e_1$, $\CC Ae_1$, and $\CC (e_1+Ae_1)$ and deduce that our initial quiver is just multiple copies of the quiver
\[\xymatrix{
\underset{\CC (1,0)}{\bullet} \ar@{^{(}->}[r] & \overset{\CC^2}{\bullet} & \ar@{_{(}->}[l] \underset{\CC(1,1)}{\bullet}\\
& \ar@{_{(}->}[u] \underset{\CC (0,1)}{\bullet} &
}\]
Indeed, note that $e_1$ and $Ae_1$ are linearly independent vectors, so we can think of $e_1=(1,0)$, $Ae_1=(0,1)$, $e_1+Ae_1=(1,1)$, and $\text{span}\{e_1,Ae_1\}=\CC^2$. It is not too hard, albeit tedious, to check that the quiver below
\[\xymatrix{
\underset{1}{\bullet} \ar[r] & \overset{2}{\bullet} & \ar[l] \underset{1}{\bullet}\\
& \underset{1}{\bullet} \ar[u] &
}\]
is indecomposable.


\section{Roots}

There is another equivalent formulation of a root system:

\begin{definition}
A \vocab{root system} $\Phi$ of a vector space $V$ is a set of vectors such that
\begin{enumerate}[(i)]
    \item the roots span $V$,
    \item the only scalar multiples of $\alpha\in\Phi$ are $\alpha$ and $-\alpha$,
    \item for every $\alpha\in\Phi$, $\Phi$ is closed under reflection across the hyperplane orthogonal to $\alpha$, i.e.
    \[\beta\mapsto \beta-\frac{2(\alpha,\beta)}{(\alpha,\alpha)}\alpha.\]
\end{enumerate}
\end{definition}

Then, it turns out that for any root system, we can \textit{pick} a set of simple roots; then, all roots will be positive or negative roots with respect to this set of simple roots.

Intuitively, we just want to pick our simple roots so that they have a large angle with respect to each other; then, all the other roots in the system will lie between them (in the positive/negative space).

\paragraph{On Example 6.4.9.} The main idea for computing roots as illustrated in this example is to choose a convenient basis of simple roots so that the inner product $B$ acts like the standard Euclidean inner product. The upside of this is that it allows us to better visualize how the root system behaves. 

\paragraph{On Remark 6.4.11.} Here, $O(\mathbb{R}^n)$ is the space of orthogonal transformations; i.e. ones that preserve the Euclidean distance. This is just because our operation $s_{\alpha}$ reflects vectors through a hyperplane, which won't affect their length. The $s_{\alpha}$ also preserve $B$, because
\begin{align*}
    B(s_{\alpha}v,s_{\alpha}v) &= B(v-B(v,\alpha)\alpha,v-B(v,\alpha)\alpha)\\
    &= B(v,v)-2B(v,B(v,\alpha)\alpha)+B(B(v,\alpha)\alpha,B(v,\alpha)\alpha)\\
    &= B(v,v)-2B(v,\alpha)B(v,\alpha)+B(v,\alpha)^2B(\alpha,\alpha)\\
    &= B(v,v)-2B(v,\alpha)^2+2B(v,\alpha)^2\\
    &= B(v,v)
\end{align*}

Consider the particular example of the Dynkin diagram $A_{N-1}$, whose lattice is embedded in $\ZZ^N$. Then recall that
\[\alpha_i=e_i-e_{i+1},\]
where $B(e_i,e_j)=\delta_{ij}$. Now,
\[s_{\alpha_i}(e_j)=e_j-\frac{2B(e_j,\alpha_i)}{B(\alpha_i,\alpha_i)}\alpha_i=\begin{cases}
e_j & \text{if }j\neq i,i+1,\\
e_{j+1} & \text{if }j=i,\\
e_{j-1} & \text{if } j=i+1.
\end{cases}\]
So, the matrix corresponding to $s_{\alpha_i}$ simply transposes the adjacent $e_i$ and $e_{i+1}$:
\[s_i=\begin{pmatrix}
1 & 0 & \cdots & 0 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 0 & 1 & \cdots & 0\\
0 & 0 & \cdots & 1 & 0 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 0 & 0 & \cdots & 1
\end{pmatrix}\]

Therefore, the Weyl group of $A_{n-1}$ is the space generated by all the transpositions, which is just the symmetric group $S^n$.

\setcounter{section}{5}

\section{Reflection Functors}

Let's give a very brief introduction to category theory and the idea of a functor. A \vocab{category} consists of objects and arrows (morphisms between objects). In particular, every object has an identity morphism to itself. 
\begin{example}
\listhack
\begin{enumerate}[(a)]
    \item The category of groups, where the arrows are homomorphisms.
    \item The category of vector spaces, where the arrows are linear maps.
\end{enumerate}
\end{example}
A \vocab{functor} $\mathcal{F}$ is a map between categories $\mathcal{C}$ and $\mathcal{D}$, which sends objects and arrows from $\mathcal{C}$ to objects and arrows from $\mathcal{D}$:
\[\mathcal{F}:\operatorname{ob}\mathcal{C}\to\operatorname{ob}\mathcal{D}, \qquad \mathcal{F}:\operatorname{arrows}\mathcal{C}\to\operatorname{arrows}\mathcal{D}.\]
This has to respect identity morphisms and composition of morphisms.

\paragraph{On Definition 6.6.3} Here, our reflection functor is from the category of representations of $Q$ to the category of representations of $\ol{Q_i}$. Let's check that this is actually a functor, i.e. if we have two representations $V$ and $W$ of a quiver $Q$ and homomorphism $f:V\to W$ between them, then the functor gives us some homomorphism of representations from $F_i^+(V)$ to $F_i^+(W)$ of $\ol{Q_i}$.

In the homomorphism $f:V\to W$, suppose $f_k$ is the map between vector spaces $V_k$ and $W_k$. We allow all the $f_k$ to be the same when $k\neq i$ because the vector spaces $V_k$ and $W_k$ don't change. We need to figure out where $f_i$ goes to. We need
\[F_i^+(f_i):\ker\left(\varphi:\bigoplus_{j\to i}V_j\to V_i\right)\to \ker\left(\psi:\bigoplus_{j\to i}W_j\to W_i\right).\]
But we can simply map any $(v_1,\dots, v_k)\in \bigoplus_{j\to i}V_j$ naturally to $(f_1(v_1),\dots,f_k(v_k))\in \bigoplus_{j\to i}W_j$; we simply need to check that if the original vector is in the kernel, i.e. $\sum \varphi_j(v_j)=0$, then the new vector is also in the kernel. Yet
\[\sum \psi_j(f_j(v_j))=\sum f_i(\varphi_j(v_j))=f_i\left(\sum \varphi_j(v_j) \right)=0,\]
since $f$ is a map of representations so it commutes with the maps in the quiver:
\[\xymatrix{
V_j \ar[r]^{\varphi_j} \ar[d]_{f_j} & V_i \ar[d]^{f_i}\\
W_j \ar[r]_{\psi_j} & W_i}\]

\paragraph{On Definition 6.6.4}
We can do a similar calculation to show that $F_i^{-}$ is a functor. Let $f\colon V\to W$ be a homomorphism, and we need to figure out where $f_i$ goes. We need
\[F_i^{-}(f_i)\colon \coker\left(\varphi\colon V_i\to \bigoplus_{j\to i}V_j\right)\to\coker\left(\psi\colon W_i\to \bigoplus_{j\to i}W_j\right).\]
For this, we simply map $(v_1,\ldots,v_k)\in (\bigoplus_{j\to i}V_j)/\img \varphi$ to $(f_1(v_1),\ldots,f_k(v_k))\in \bigoplus_{j\to i}W_j$. To show that this is in the cokernel of $\psi$, check that
\begin{align*}
(v_1,\ldots,v_k)+(\varphi_1(v_1'),\ldots,\varphi_k(v_k')) &\mapsto (f_1(v_1+\varphi_1(v_1')),\ldots,f_k(v_k+\varphi_k(v_k')))\\
&= (f_1(v_1)+\psi_1(f_i(v_1')),\ldots,f_k(v_k)+\psi_k(f_k(v_k')))\\
&= (f_1(v_1),\ldots,f_k(v_k))+(\psi_1(f_1(v_1')),\ldots,\psi_k(f_k(v_k')))
\end{align*}
due to the following commutative diagram
\[\xymatrix{
V_i \ar[r]^{\varphi_j} \ar[d]_{f_i} & V_j \ar[d]^{f_j}\\
W_i \ar[r]_{\psi_j} & W_j}\]

\paragraph{On Proposition 6.6.6}
Etingof does not show why applying $F_i^{-}F_i^{+}$ doesn't change the maps between the vector spaces. We will show this here.

First, consider the $F_i^{+}$ reflection functor. It leaves the $V_j$ for $j\neq i$ the same and replaces $V_i$ with $\ol{V_i}^{+}=\ker \varphi$. We only need to study the linear maps going into $V_i$. Here is how $F_i^{+}$ affects the linear maps $\varphi_j\colon V_j\to V_i$ .
\[\xymatrix{
\ol{V_i}^{+} \ar@{^{(}->}[r]^{\iota} \ar[rd]_{\ol{\varphi_j}^{+}} & \bigoplus_{j\to i}V_j \ar[d]^{\pi_j}\\
 & V_j}\]
And here is how $F_i^{-}$ affects the linear maps $\ol{\varphi_j}\colon V_i\to V_j$. 
\[\xymatrix{
\coker \ol{\varphi} & \ar@{->>}[l]  \bigoplus_{j\to i}V_j\\
& V_j \ar[lu]^{\ol{\varphi_j}^{-}} \ar@{^{(}->}[u]}\]
Then, when $\varphi$ is surjective, we proved that $\coker \ol{\varphi}^{+}=V_i$, so the following diagram commutes. 
\[\xymatrix{0 \ar[r] & \ol{V_i}^{+} \ar@{^{(}->}[r]^{\ol{\varphi}^{+}} \ar[rd]_{\ol{\varphi_j}^{+}} & \bigoplus_{j\to i} V_j \ar@{->>}[r]^{\varphi} \ar@<-1ex>[d]_{\pi_j} & V_i \ar[r] & 0\\
& & V_j \ar@{^{(}->}@<-.9ex>[u]_{\iota_j} \ar[ru]_{\varphi_j} & }\]
However, according to this diagram, we have linear maps $\ol{\varphi_j}^{+}$ from $\ol{V_i}^{+}$ into $V_j$ (this is the left hand side of the diagram). But since $V_i=\coker \ol{\varphi}^{+}$, the commutative diagram for $F_i^{-}$ tells us that the map $\ol{\varphi_j}^{+}$ just becomes $\varphi_j$, which is the original map that we wanted.

\paragraph{On Proposition 6.6.7} The only nontrivial part is that $F_i^-(X\oplus Y)=F_i^-(X)\oplus F_i^-(Y)$. But if we have maps $X\to Z$ and $Y\to T$, and we look at their direct sum $X\oplus Y\to Z\oplus T$, then the (co)kernel of this direct sum map is the direct sum of the two original (co)kernels.


\section{Coxeter elements}
Given some labeling $1,\ldots,n$ of the vertices of the quiver $Q$, we define the \textbf{Coxeter element} of $Q$ to be the element
\[c=s_1s_2\cdots s_n\]
in the Weyl group $W$ (it's just the composition of all the simple reflections). Recall that $W$ is a finite group because there are finitely many roots. Note that for different labelings of the quiver, we could get different Coxeter elements. 

\section{Proof of Gabriel's theorem}
\paragraph{On Theorem 6.8.1.}
For the representation $V^{(i)}$, the ``appropriate vertex'' which Etingof describes is the sink vertex which we will flip to get $V^{(i+1)}$. We want to prove that there is some $i$ for which $V^{(i)}$ is NOT surjective at its appropriate vertex, and we do this by assuming for the sake of contradiction that all $V^{(i)}$ are surjective at their appropriate vertex.

After getting a contradiction using the Coxeter element, we get that some $V^{(i)}$ is not surjective at the appropriate vertex. And since it's indecomposable, an earlier result implies that one vector space has dimension one and all other vector spaces has dimension zero. So the dimension vector is some simple root. 



\newpage

\nocite{*}
\bibliographystyle{plain}
\bibliography{bibfile}


\end{document}
